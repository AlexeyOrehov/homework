{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d3645d6",
   "metadata": {},
   "source": [
    "# Шпаргалка по Метрикам Классификации и Регрессии\n",
    "\n",
    "Эта шпаргалка содержит основные метрики, используемые для оценки моделей машинного обучения в задачах классификации и регрессии.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Метрики Классификации\n",
    "\n",
    "Метрики классификации используются для оценки производительности моделей, предсказывающих дискретные классы.\n",
    "\n",
    "### 1.1. Матрица Ошибок (Confusion Matrix)\n",
    "\n",
    "Матрица ошибок является основой для многих метрик классификации. Она показывает количество истинных и ложных срабатываний модели для каждого класса.\n",
    "\n",
    "|             | Предсказано: Положительный (1) | Предсказано: Отрицательный (0) |\n",
    "| :---------- | :----------------------------- | :----------------------------- |\n",
    "| **Реально: Положительный (1)** | True Positive (TP)             | False Negative (FN)            |\n",
    "| **Реально: Отрицательный (0)** | False Positive (FP)            | True Negative (TN)             |\n",
    "\n",
    "* **TP (True Positive):** Истинно-положительные. Модель правильно предсказала положительный класс.\n",
    "* **FN (False Negative):** Ложно-отрицательные. Модель ошибочно предсказала отрицательный класс, когда на самом деле был положительный. (Ошибка I рода)\n",
    "* **FP (False Positive):** Ложно-положительные. Модель ошибочно предсказала положительный класс, когда на самом деле был отрицательный. (Ошибка II рода)\n",
    "* **TN (True Negative):** Истинно-отрицательные. Модель правильно предсказала отрицательный класс.\n",
    "\n",
    "### 1.2. Accuracy (Точность)\n",
    "\n",
    "**Описание:** Доля правильно классифицированных образцов от общего числа образцов.\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "**Пример:** Если из 100 предсказаний 90 были верными (50 TP и 40 TN), а 10 неверными (5 FP и 5 FN), то `Accuracy = (50 + 40) / 100 = 0.9`.\n",
    "\n",
    "**Когда использовать:** Хорошо подходит для сбалансированных наборов данных, где количество образцов каждого класса примерно одинаково.\n",
    "\n",
    "**Когда не использовать:** Плохо подходит для несбалансированных наборов данных, так как высокий Accuracy может быть обманчивым (например, модель, которая всегда предсказывает мажоритарный класс, будет иметь высокий Accuracy).\n",
    "\n",
    "### 1.3. Precision (Точность, Процент правильных предсказаний положительного класса)\n",
    "\n",
    "**Описание:** Доля истинно-положительных предсказаний от всех предсказаний, которые модель сделала как положительные. Отвечает на вопрос: \"Из всех предсказаний 'положительный', сколько из них действительно были положительными?\"\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n",
    "$$\n",
    "\n",
    "**Пример:** Если модель предсказала 60 положительных случаев, и 50 из них были истинно-положительными (а 10 ложно-положительными), то `Precision = 50 / (50 + 10) = 0.83`.\n",
    "\n",
    "**Когда использовать:** Важно, когда стоимость ложно-положительных предсказаний высока (например, система обнаружения спама, где ложно-положительное предсказание означает удаление важного письма).\n",
    "\n",
    "### 1.4. Recall (Полнота, Чувствительность, Доля истинно-положительных)\n",
    "\n",
    "**Описание:** Доля истинно-положительных предсказаний от всех реальных положительных образцов. Отвечает на вопрос: \"Из всех реальных положительных случаев, сколько из них модель смогла обнаружить?\"\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n",
    "$$\n",
    "\n",
    "**Пример:** Если в реальном наборе данных было 70 положительных случаев, и модель предсказала 50 из них как положительные (а 20 пропустила как ложно-отрицательные), то `Recall = 50 / (50 + 20) = 0.71`.\n",
    "\n",
    "**Когда использовать:** Важно, когда стоимость ложно-отрицательных предсказаний высока (например, медицинская диагностика заболеваний, где пропуск больного человека может быть фатальным).\n",
    "\n",
    "### 1.5. F1-Score (F-мера)\n",
    "\n",
    "**Описание:** Гармоническое среднее Precision и Recall. Представляет собой баланс между Precision и Recall.\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{F1-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**Пример:** Если `Precision = 0.8` и `Recall = 0.7`, то `F1-Score = 2 * (0.8 * 0.7) / (0.8 + 0.7) = 2 * 0.56 / 1.5 = 1.12 / 1.5 = 0.747`.\n",
    "\n",
    "**Когда использовать:** Когда нужен баланс между Precision и Recall, особенно для несбалансированных наборов данных.\n",
    "\n",
    "### 1.6. ROC Curve (Receiver Operating Characteristic Curve) и AUC (Area Under the Curve)\n",
    "\n",
    "**Описание:**\n",
    "\n",
    "* **ROC Curve:** График, показывающий производительность классификационной модели при различных порогах классификации. Он отображает зависимость True Positive Rate (TPR, то же самое, что Recall) от False Positive Rate (FPR) при различных порогах.\n",
    "    * **TPR (True Positive Rate):** $ \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $\n",
    "    * **FPR (False Positive Rate):** $ \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} $\n",
    "* **AUC:** Площадь под ROC-кривой. Представляет собой агрегированную меру производительности модели во всех возможных порогах классификации. Значение AUC от 0 до 1.\n",
    "\n",
    "**Интерпретация AUC:**\n",
    "\n",
    "* **0.5:** Модель работает не лучше случайного угадывания.\n",
    "* **1.0:** Идеальная модель.\n",
    "* Чем ближе AUC к 1, тем лучше модель различает положительный и отрицательный классы.\n",
    "\n",
    "**Пример:** Если AUC для модели равен 0.92, это означает, что модель хорошо разделяет классы.\n",
    "\n",
    "**Когда использовать:** Когда важно оценить способность модели различать классы при различных порогах. Особенно полезно для несбалансированных данных и для сравнения моделей.\n",
    "\n",
    "### 1.7. Log Loss (Логарифмическая функция потерь, Кросс-энтропия)\n",
    "\n",
    "**Описание:** Измеряет неопределенность предсказаний модели, сравнивая предсказанные вероятности с истинными метками классов. Чем меньше Log Loss, тем лучше модель.\n",
    "\n",
    "**Формула (для бинарной классификации):**\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = - \\frac{1}{N} \\sum_{i=1}^{N} [y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)]\n",
    "$$\n",
    "\n",
    "Где:\n",
    "* $N$ - количество образцов.\n",
    "* $y_i$ - истинная метка класса для образца $i$ (0 или 1).\n",
    "* $p_i$ - предсказанная вероятность того, что образец $i$ относится к классу 1.\n",
    "\n",
    "**Пример:** Если модель предсказывает вероятность 0.9 для класса 1, а истинная метка 1, Log Loss будет низким. Если модель предсказывает 0.1, а истинная метка 1, Log Loss будет высоким.\n",
    "\n",
    "**Когда использовать:** Когда важны калиброванные вероятности предсказаний (например, для предсказаний риска). Чувствителен к ошибочным предсказаниям с высокой уверенностью.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Метрики Регрессии\n",
    "\n",
    "Метрики регрессии используются для оценки производительности моделей, предсказывающих непрерывные числовые значения.\n",
    "\n",
    "Пусть $y_i$ - истинное значение, а $\\hat{y}_i$ - предсказанное значение для $i$-го образца.\n",
    "\n",
    "### 2.1. MAE (Mean Absolute Error - Средняя Абсолютная Ошибка)\n",
    "\n",
    "**Описание:** Среднее арифметическое абсолютных разностей между истинными и предсказанными значениями.\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "**Пример:** Если истинные значения [10, 20, 30] и предсказанные [12, 18, 33], то абсолютные ошибки [2, 2, 3]. `MAE = (2 + 2 + 3) / 3 = 7 / 3 = 2.33`.\n",
    "\n",
    "**Когда использовать:** Менее чувствителен к выбросам, чем MSE. Ошибка измеряется в тех же единицах, что и целевая переменная, что делает ее более интерпретируемой.\n",
    "\n",
    "### 2.2. MSE (Mean Squared Error - Средняя Квадратичная Ошибка)\n",
    "\n",
    "**Описание:** Среднее арифметическое квадратов разностей между истинными и предсказанными значениями.\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "**Пример:** Если истинные значения [10, 20, 30] и предсказанные [12, 18, 33], то квадраты ошибок [4, 4, 9]. `MSE = (4 + 4 + 9) / 3 = 17 / 3 = 5.67`.\n",
    "\n",
    "**Когда использовать:** Позволяет сильно \"наказывать\" большие ошибки благодаря возведению в квадрат. Широко используется из-за своих математических свойств (дифференцируемость).\n",
    "\n",
    "**Когда не использовать:** Чувствителен к выбросам, так как большие ошибки оказывают непропорционально большое влияние. Единицы измерения ошибки не совпадают с единицами измерения целевой переменной.\n",
    "\n",
    "### 2.3. RMSE (Root Mean Squared Error - Корень из Средней Квадратичной Ошибки)\n",
    "\n",
    "**Описание:** Квадратный корень из MSE.\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "**Пример:** Используя предыдущий пример `MSE = 5.67`, `RMSE = \\sqrt{5.67} \\approx 2.38`.\n",
    "\n",
    "**Когда использовать:** Как и MAE, RMSE измеряется в тех же единицах, что и целевая переменная, что делает ее более интерпретируемой, чем MSE. Он также наказывает большие ошибки, но в меньшей степени, чем чистый MSE.\n",
    "\n",
    "### 2.4. R-squared ($R^2$, Coefficient of Determination - Коэффициент Детерминации)\n",
    "\n",
    "**Описание:** Измеряет долю дисперсии зависимой переменной, которая объясняется моделью. Значение от 0 до 1 (иногда может быть отрицательным для очень плохих моделей).\n",
    "\n",
    "**Формула:**\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{N} (y_i - \\bar{y})^2}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "* $\\bar{y}$ - среднее значение истинных значений $y_i$.\n",
    "\n",
    "**Интерпретация:**\n",
    "* **1:** Модель идеально объясняет дисперсию.\n",
    "* **0:** Модель не объясняет дисперсию (то же самое, что предсказывать среднее).\n",
    "* **< 0:** Модель работает хуже, чем просто предсказание среднего значения.\n",
    "\n",
    "**Пример:** Если $R^2 = 0.85$, это означает, что 85% дисперсии в целевой переменной объясняется моделью.\n",
    "\n",
    "**Когда использовать:** Отличная метрика для понимания того, насколько хорошо модель соответствует данным. Позволяет сравнивать модели, обученные на разных наборах данных.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
